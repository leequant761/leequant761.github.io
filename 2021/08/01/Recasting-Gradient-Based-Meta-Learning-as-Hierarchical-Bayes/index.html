<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"leequant761.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="0. AbstractMeta-learning(ML)은 agent가 이전의 episode를 기반으로 새로운 task에서 빠르게 적응하도록 한다. Hierarchical Bayesian Model은 task들 간의 공유되는 parameter의 분포를 추론하는 문제로 ML을 formalize하는 이론적 framework를 제공한다. 우리는 MAML을 probab">
<meta property="og:type" content="article">
<meta property="og:title" content="Recasting Gradient-Based Meta-Learning as Hierarchical Bayes">
<meta property="og:url" content="https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/index.html">
<meta property="og:site_name" content="Study Repo">
<meta property="og:description" content="0. AbstractMeta-learning(ML)은 agent가 이전의 episode를 기반으로 새로운 task에서 빠르게 적응하도록 한다. Hierarchical Bayesian Model은 task들 간의 공유되는 parameter의 분포를 추론하는 문제로 ML을 formalize하는 이론적 framework를 제공한다. 우리는 MAML을 probab">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731122937369.png">
<meta property="og:image" content="https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731134032852.png">
<meta property="og:image" content="https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731180638986.png">
<meta property="og:image" content="https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731181921053.png">
<meta property="article:published_time" content="2021-08-01T10:16:26.000Z">
<meta property="article:modified_time" content="2021-08-01T10:20:46.676Z">
<meta property="article:author" content="JaeHyun Lee">
<meta property="article:tag" content="paper review">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731122937369.png">

<link rel="canonical" href="https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Recasting Gradient-Based Meta-Learning as Hierarchical Bayes | Study Repo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Study Repo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JaeHyun Lee">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Study Repo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Recasting Gradient-Based Meta-Learning as Hierarchical Bayes
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-08-01 19:16:26 / Modified: 19:20:46" itemprop="dateCreated datePublished" datetime="2021-08-01T19:16:26+09:00">2021-08-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Meta-learning/" itemprop="url" rel="index"><span itemprop="name">Meta-learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="0-Abstract"><a href="#0-Abstract" class="headerlink" title="0. Abstract"></a>0. Abstract</h1><p>Meta-learning(ML)은 agent가 이전의 episode를 기반으로 새로운 task에서 빠르게 적응하도록 한다.</p>
<p>Hierarchical Bayesian Model은 task들 간의 공유되는 parameter의 분포를 추론하는 문제로 ML을 formalize하는 이론적 framework를 제공한다.</p>
<p>우리는 MAML을 probabilistic inference in HBM으로 formalize한다.</p>
<p>기존의 HBM을 활용한 ML 방법들과 달리, MAML은 task-specific parameter의 posterior inference를 GD로 대체하기에 복잡한 함수로의 적용이 가능하다.</p>
<p>MAML을 HBM으로 받아들이면,</p>
<ol>
<li>ML의 작동원리를 이해할 수 있고</li>
<li>Efficient inference의 기회를 제공한다.</li>
</ol>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>사람은 novel problem이 빠르게 해결할 수 있는 능력이 있다.</p>
<p>그러한 빠른 adaptation은 이전의 학습 경험을 활용함으로 이뤄진다.</p>
<p>ML의 meta-knowledge는 이전의 학습경험을 활용해서 AI agent가 제한된 data로 학습할 수 있음을 하게 해준다.</p>
<p>머신러닝에서, ML은 새로운 task에서의 학습의 효율을 향상시키는 inductive bias로 작동할 수 있는 domain-general information을 extract하여 formulate될 수 있다.</p>
<p>이 inductive bias는 다양하게 구현되어왔다.</p>
<ol>
<li>HBM : Task-specific parameter를 제한하는 hyperparameter로서</li>
<li>Learned-Metric space : …</li>
<li>RNN의 weight을 meta-knowledge로 활용하여 hidden-state를 활용함으로서</li>
<li>Optimization algorithm의 parameter로서</li>
</ol>
<hr>
<p>MAML은 4에 속하며, model의 initial parameter를 fast adaptation을 위한 inductive bias로 활용하였다.</p>
<img src="/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731122937369.png" class="">
<p>MAML의 inductive bias는 empirically evaluate되어왔다.</p>
<hr>
<p>이 논문에선, MAML을,<br>HBM에서의 task들 간에 공유하는 parameter의 prior distribution을 추론하는 문제로 이해됨을 보인다.</p>
<p>MAML의 학습된 prior는 implicit predictive distribution over task-specific parameter에 기반하여 빠르게 unseen task에 adapt한다.</p>
<p>MAML을 HBM으로 해석하게 되면, bayesian posterior estimation으로 부터 insight를 가져와서, MAML을 향상시킬 수 있게 된다.</p>
<h1 id="2-Meta-Learning-Formulation"><a href="#2-Meta-Learning-Formulation" class="headerlink" title="2. Meta-Learning Formulation"></a>2. Meta-Learning Formulation</h1><p>Meta-learner의 목표는 다양한 task의 경험을 통해 task-general knowledge를 extract하는 것이다.</p>
<p>이를 prior knowledge로 활용하여, learner는 새로운 task에 빠르게 adapt할 수 있다.</p>
<p>Task들이 common structure를 공유하기 때문에 한 task의 학습이 다른 task의 학습에 도움이 될 수 있다.</p>
<p>Meta-learner의 objective function은 $\mathcal T \sim p(\mathcal T)$에서의 task-specific performance를 최대화하는 것이다.</p>
<hr>
<p>ML을 2가지 방법으로 formulate하겠다.</p>
<ol>
<li>gradient-based hyperparameter optimization</li>
<li>probabilistic inference in HBM</li>
</ol>
<p>이들은 따로 발전되어왔지만, 3.1에서 이들을 엮어보겠다.</p>
<h2 id="2-1-Meta-Learning-as-Gradient-Based-Hyperparameter-Optimization"><a href="#2-1-Meta-Learning-as-Gradient-Based-Hyperparameter-Optimization" class="headerlink" title="2.1 Meta-Learning as Gradient-Based Hyperparameter Optimization"></a>2.1 Meta-Learning as Gradient-Based Hyperparameter Optimization</h2><p>Parametric meta-learner는 novel task를 만났을 때, 이에 맞는 task-specific parameter $\boldsymbol \phi$를 찾는걸 쉽게해주는 $\boldsymbol \theta$를 찾는 걸 도와준다.</p>
<p>Gradient method들을 사용하는 이전의 방법들과 달리 MAML은 $\boldsymbol \phi$와 $\boldsymbol \theta$가 같은 space에 있는 방법이다.</p>
<script type="math/tex; mode=display">
\mathcal{L}(\boldsymbol{\theta})=\frac{1}{J} \sum_{j}\left[\frac{1}{M} \sum_{m}-\log p(\mathbf{x}_{j_{N+m}} \mid \underbrace{\boldsymbol{\theta}-\alpha \nabla_{\boldsymbol{\theta}} \frac{1}{N} \sum_{n}-\log p\left(\mathbf{x}_{j_{n}} \mid \boldsymbol{\theta}\right)}_{\boldsymbol{\phi}_{j}})\right]</script><p>where $\boldsymbol \phi_{j}$ is the updated task-specific parameter</p>
<h2 id="2-2-Meta-Learning-as-Hierarchical-Bayesian-Inference"><a href="#2-2-Meta-Learning-as-Hierarchical-Bayesian-Inference" class="headerlink" title="2.2 Meta-Learning as Hierarchical Bayesian Inference"></a>2.2 Meta-Learning as Hierarchical Bayesian Inference</h2><p>메타러닝을 HBM에서의 probabilistic inference로 볼 수도 있다.</p>
<img src="/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731134032852.png" class="">
<p>메타러닝에서, task-specific parameter $\boldsymbol \phi _j$는 구분되었지만 이의 estimation이 $\left\{\boldsymbol \phi_{j^{\prime}} \mid j^{\prime} \neq j\right\}$ estimation에 영향을 줘야한다.</p>
<p>이를 위해 meta-level parameter $\boldsymbol \theta$를 도입하게 된다.</p>
<p>이 구조에서 $\boldsymbol \theta$에 대한 estimate은 $\boldsymbol \phi_j$의 inference에 제한을 주게된다.</p>
<script type="math/tex; mode=display">
p(\mathbf{X} \mid \boldsymbol{\theta})=\prod_{j}\left(\int p\left(\mathbf{x}_{j_{1}}, \ldots, \mathbf{x}_{j_{N}} \mid \boldsymbol \phi_{j}\right) p\left(\boldsymbol \phi_{j} \mid \boldsymbol{\theta}\right) \mathrm{d} \boldsymbol{\phi}_{j}\right)</script><p>이를 maximizing하는 것은 empirical Bayes라고 알려졌다.</p>
<blockquote>
<p>Prior distribution의 parameter $\theta$를 estimate하는 데에 data를 사용했다는 관점에서</p>
</blockquote>
<p>HBM은 transfer learning과 domain adaptation에서 사용의 역사가 있다.</p>
<p>하지만 HBM은 inference procedure를 제공하지 않고 복잡한 모델에 대해서 tractability를 제공하지 않는다.</p>
<h1 id="3-Linking-Gradient-Based-Meta-Learning-amp-Hierarchical-Bayes"><a href="#3-Linking-Gradient-Based-Meta-Learning-amp-Hierarchical-Bayes" class="headerlink" title="3. Linking Gradient-Based Meta-Learning &amp; Hierarchical Bayes"></a>3. Linking Gradient-Based Meta-Learning &amp; Hierarchical Bayes</h1><p>MAML이 empirical Bayes in HBM로 이해될 수 있음을 보여서, 위의 둘을 연결짓겠다.</p>
<p>MAML의 meta-learning은 Task-specific prior distribution $p\left(\boldsymbol{\phi}_{j} \mid \boldsymbol{\theta}\right)$을 선택하는 과정이다.</p>
<h2 id="3-1-MAML-as-Empirical-Bayes"><a href="#3-1-MAML-as-Empirical-Bayes" class="headerlink" title="3.1 MAML as Empirical Bayes"></a>3.1 MAML as Empirical Bayes</h2><p>일반적으로 marginalization over task-specific parameters $\boldsymbol \phi_j$은 tractable하지 않다.</p>
<p>대안으로, point estimate $\hat {\boldsymbol \phi_j}$으로 대체하는 것이다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\mathbf{X} \mid \boldsymbol{\theta}) &=\prod_{j}\left(\int p\left(\mathbf{x}_{j_{1}}, \ldots, \mathbf{x}_{j_{N}} \mid \boldsymbol \phi_{j}\right) p\left(\boldsymbol \phi_{j} \mid \boldsymbol{\theta}\right) \mathrm{d} \boldsymbol{\phi}_{j}\right)
\\
&\approx \prod_{j} p\left(\mathbf{x}_{j_{1}}, \ldots, \mathbf{x}_{j_{N}} \mid \hat{\boldsymbol \phi_{j}}\right)
\\

\\
-\log p(\mathbf{X} \mid \boldsymbol{\theta}) & \approx \sum_{j}\left[-\log p\left(\mathbf{x}_{j_{N+1}}, \ldots \mathbf{x}_{j_{N+M}} \mid \hat{\boldsymbol{\phi}}_{j}\right)\right]
\end{aligned}</script><p>$\hat{\boldsymbol{\phi}}_{j}=\boldsymbol{\theta}+\alpha \nabla_{\boldsymbol{\theta}} \log p\left(\mathbf{x}_{j_{1}}, \ldots, \mathbf{x}_{j_{N}} \mid \boldsymbol{\theta}\right)$로 두게 되면 이는 one-step MAML의 objective가 된다.</p>
<p>$\hat{\boldsymbol{\phi}}_{j}$는 $\boldsymbol \theta$에서 크게 벗어나지 않는 선에서, $-\log p(\mathbf{X} \mid \boldsymbol{\theta}) $를 줄이는 것에 대응된다.</p>
<hr>
<p>Linear regression case에서 이러한 trade-off를 HBM으로 formalize할 수 있다.</p>
<p>Linear model에서 early stopping of GD의 옵션은 특정 prior(step의 횟수 direction에 의존하는) 하에서 MAP를 구하는 것이다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{\phi}_{(k)} &=\boldsymbol{\phi}_{(k-1)}-\alpha \nabla_{\boldsymbol{\phi}}\left[\|\mathbf{y}-\mathbf{X} \boldsymbol{\phi}\|_{2}^{2}\right]_{\phi=\boldsymbol{\phi}_{(k-1)}}
\\
&=\boldsymbol \phi_{(k-1)}-\alpha \mathbf{X}^{\mathrm{T}}\left(\mathbf{X} \boldsymbol \phi_{(k-1)}-\mathbf{y}\right)

\end{aligned}</script><p><a href="">Santos, 1996</a>은 $\boldsymbol \phi_{(0)}=\boldsymbol \theta$에서 시작하는 것은 $\min \left(|\mathbf{y}-\mathbf{X} \boldsymbol{\phi}|_{2}^{2}+|\boldsymbol{\theta}-\boldsymbol{\phi}|_{\mathbf{Q}}^{2}\right)$를 최적화하는 것이라고 했다.</p>
<blockquote>
<p>이 부분은 수식이 애매하게 넘어감.</p>
</blockquote>
<p>$\min \left(|\mathbf{y}-\mathbf{X} \boldsymbol{\phi}|_{2}^{2}+|\boldsymbol{\theta}-\boldsymbol{\phi}|_{\mathbf{Q}}^{2}\right)$는 MAP를 구하는 것에 대응된다.</p>
<ul>
<li>$\boldsymbol \phi \sim \mathcal{N}(\boldsymbol \phi ; \boldsymbol{\theta}, \mathbf{Q})$</li>
<li>$ \mathbf y  \mid \mathbf X, \boldsymbol \phi \sim \mathcal{N}(\mathbf{y} ; \mathbf{X} \boldsymbol \phi, \mathbb{I})$</li>
</ul>
<p>그러므로 Linear regression에서의 MAML은 implicit prior하에서 empirical Bayes를 하는 것에 대응된다.</p>
<hr>
<p>Nonlinear의 경우엔, MAML은 empirical Bayes procedure와 동일하지만 $\boldsymbol \phi$가 MAP에 대응되진 않는다.</p>
<blockquote>
<p>그 뒤에 뭐라 말을 했는데 그런가보다 하고 넘어감</p>
<p>MAP 대신에 implicit MAP라는 표현을 사용함</p>
</blockquote>
<p>결국 MAML은 MAP로 marginal likelihood를 approximate하고 난 뒤에 $\boldsymbol \theta$를 update하는 것이다.</p>
<script type="math/tex; mode=display">
\mathbb{E}_{\mathbf{x} \sim p_{\mathcal{T}_{j}}(\mathbf{x})}[-\log p(\mathbf{x} \mid \boldsymbol{\theta})] \approx \frac{1}{M} \sum_{m}-\log p\left(\mathbf{x}_{j_{N+m}} \mid \hat{\boldsymbol{\phi}}_{j}\right)</script><hr>
<img src="/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731180638986.png" class="">
<img src="/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/image-20210731181921053.png" class="">
<h2 id="3-2-The-Prior-over-Task-Specific-Parameters"><a href="#3-2-The-Prior-over-Task-Specific-Parameters" class="headerlink" title="3.2 The Prior over Task-Specific Parameters"></a>3.2 The Prior over Task-Specific Parameters</h2><p>3.1에서 early stopping(fast adaptation)은 prior selection과 동일하다고 결론지었다.</p>
<p>Let $\phi^\star$ be a minima of fast adaptation objective $\ell(\phi)=-\log p\left(\mathbf{x}_{1} \ldots, \mathbf{x}_{N} \mid \phi\right)$</p>
<p>Consider second-order approximation</p>
<script type="math/tex; mode=display">
\ell(\boldsymbol\phi) \approx \tilde{\ell}(\boldsymbol\phi):=\frac{1}{2}\left\|\boldsymbol\phi-\boldsymbol\phi^{*}\right\|_{\mathbf{H}^{-1}}^{2}+\ell\left(\boldsymbol\phi^{*}\right)</script><p>where $\mathbf{H}=\nabla_{\boldsymbol{\phi}}^{2} \ell\left(\boldsymbol{\phi}^{*}\right)$ is assumed to be positive definite.</p>
<p>Consider the gradient descent with curvature matrix $\mathcal B$</p>
<script type="math/tex; mode=display">
\boldsymbol\phi_{(k)}=\boldsymbol\phi_{(k-1)}-\mathcal{B} \nabla_{\boldsymbol\phi} \tilde{\ell}\left(\boldsymbol\phi_{(k-1)}\right)</script><p>If $\mathcal B$ is diagonal, then the update corresponds to a Newton method with a diagonal approximation.</p>
<p>Meta-learned matrix $\mathcal B$는 fast adaptation prior $p(\boldsymbol \phi \mid \boldsymbol \theta)$의 covariance matrix에 task-general information을 담는 것이다.</p>
<p>위의 GD는 w/ $\boldsymbol \phi_{(0)} = \boldsymbol \theta$</p>
<script type="math/tex; mode=display">
\min \left(\left\|\boldsymbol \phi-\boldsymbol\phi^{*}\right\|_{\mathbf{H}^{-1}}^{2}+\left\|\boldsymbol\theta -\boldsymbol\phi\right\|_{\mathbf{Q}}^{2}\right)</script><p>을 푸는 것이다.</p>
<ul>
<li><p>$\boldsymbol \phi \sim \mathcal{N}(\boldsymbol \phi ; \boldsymbol{\theta}, \mathbf{Q})$</p>
<ul>
<li><p>$\mathbf{Q}=…$</p>
<blockquote>
<p>잘 모르겠음 이해하려면 Santos 1996 읽어야 할 듯</p>
</blockquote>
</li>
</ul>
</li>
<li><p>$\left|\boldsymbol \phi-\boldsymbol\phi^{*}\right|_{\mathbf{H}^{-1}}^{2}$는 뭐에 대응하는 것 인지 언급없음</p>
</li>
</ul>
<h1 id="4-Improving-MAML"><a href="#4-Improving-MAML" class="headerlink" title="4. Improving MAML"></a>4. Improving MAML</h1><p>MAML을 HBM의 empirical Bayes로 받아들이면 이를 발전시킬 수 있다.</p>
<h2 id="4-1-Laplace’s-Method-of-Integration"><a href="#4-1-Laplace’s-Method-of-Integration" class="headerlink" title="4.1 Laplace’s Method of Integration"></a>4.1 Laplace’s Method of Integration</h2><p>MAML은 empirical Bayes w/ point estimate task-specific parameter in HBM이다.</p>
<p>그러나, point estimate의 사용은 부정확한 approximate이 될 수 있다.</p>
<script type="math/tex; mode=display">
-\log p(\mathbf{X} \mid \boldsymbol{\theta}) \approx \sum_{j}\left[-\log p\left(\mathbf{x}_{j_{N+1}}, \ldots \mathbf{x}_{j_{N+M}} \mid \hat{\boldsymbol{\phi}}_{j}\right)\right]</script><p>특히, posterior $\boldsymbol \phi$가 그렇게 sharp하지 않을 수록.</p>
<p>Laplace approximation은 이 경우에 적용이 가능하다.</p>
<hr>
<p>Suppose function $f(x)$ has a unique global maximum at $x_0$</p>
<p>Let $M$ be a constant and consider the following two eqns</p>
<ul>
<li>$g(x)=M f(x)$<ul>
<li>$\frac{g\left(x_{0}\right)}{g(x)}=\frac{M f\left(x_{0}\right)}{M f(x)}=\frac{f\left(x_{0}\right)}{f(x)}$</li>
</ul>
</li>
<li>$h(x)=e^{M f(x)}$<ul>
<li>$\frac{h\left(x_{0}\right)}{h(x)}=\frac{e^{M f\left(x_{0}\right)}}{e^{M f(x)}}=e^{M\left(f\left(x_{0}\right)-f(x)\right)}$</li>
</ul>
</li>
</ul>
<p>As $M$ increases, the ratio for $h$ will grow exponentially, but not $g$</p>
<p>즉, $h$의 적분은 거의 $x_0$의 neighborhood에서 온다.</p>
<script type="math/tex; mode=display">
f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{1}{2} f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2}+R
\\
f(x) \approx f\left(x_{0}\right)-\frac{1}{2}\left|f^{\prime \prime}\left(x_{0}\right)\right|\left(x-x_{0}\right)^{2} \quad \text{for close $x$ to $x_0$}</script><script type="math/tex; mode=display">
\begin{aligned}
\int_{a}^{b} e^{M f(x)} d x &= e^{M f\left(x_{0}\right)} \int_{a}^{b} e^{-\frac{1}{2} M\left|f^{\prime \prime}\left(x_{0}\right)\right|\left(x-x_{0}\right)^{2}} d x \quad \text{as $M \rightarrow \infty$}
\\
&= \sqrt{\frac{2 \pi}{M\left|f^{\prime \prime}\left(x_{0}\right)\right|}} e^{M f\left(x_{0}\right)} \quad \because \text{Gaussian normalization}
\end{aligned}</script><hr>
<p>이 근사를 task-specific parameter에 적용할 경우,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int p\left(\mathbf{X}_{j} \mid \boldsymbol{\phi}_{j}\right) p\left(\boldsymbol{\phi}_{j} \mid \boldsymbol{\theta}\right) \mathrm{d} \boldsymbol{\phi}_{j} 
& = \int \exp \left(\log p\left(\mathbf{X}_{j} \mid \boldsymbol{\phi}_{j}\right) + \log p\left(\boldsymbol{\phi}_{j} \mid \boldsymbol{\theta}\right)\right)  \mathrm{d} \boldsymbol{\phi}_{j}
\\
&= \int \exp \left(\log p\left(\boldsymbol{\phi}_{j} \mid \mathbf X_j , \boldsymbol{\theta}\right)\right)  \mathrm{d} \boldsymbol{\phi}_{j} + C(\theta)

\\
&\approx p\left(\mathbf{X}_{j} \mid \boldsymbol{\phi}_{j}^{*}\right) p\left(\boldsymbol{\phi}_{j}^{*} \mid \boldsymbol{\theta}\right) \operatorname{det}\left(\mathbf{H}_{j} / 2 \pi\right)^{-\frac{1}{2}}
\end{aligned}</script><p>where $\mathbf{H}_{j}$ is the Hessian matrix of second derivatives of posterior.</p>
<p>Then, fast adaptation objective will be</p>
<script type="math/tex; mode=display">
-\log p(\mathbf{X} \mid \boldsymbol{\theta}) \approx \sum_{j}\left[-\log p\left(\mathbf{X}_{j} \mid \hat{\boldsymbol{\phi}}_{j}\right)-\log p\left(\hat{\boldsymbol{\phi}}_{j} \mid \boldsymbol{\theta}\right)+\frac{1}{2} \log \operatorname{det}\left(\mathbf{H}_{j}\right)\right]</script><p>Laplace approximation을 하게 되면 model의 complexity를 penalize하는 term $\frac{1}{2} \log \operatorname{det}\left(\mathbf{H}_{j}\right)$이 추가된다.</p>
<h2 id="4-2-Using-Curvature-Information-to-Improve"><a href="#4-2-Using-Curvature-Information-to-Improve" class="headerlink" title="4.2 Using Curvature Information to Improve"></a>4.2 Using Curvature Information to Improve</h2><p>Early stopping을 대변하는 prior $p(\boldsymbol \phi \mid \boldsymbol \theta)$가 implicit하기에 구할 수 있는 대상이 아니다.</p>
<p>그래서 그냥 간단한 Gaussian with $\tau$ precision으로 대체한다.</p>
<p>Hessian의 determinant 계산의 efficiency를 위해 Kronecker-factored approximate curvature (K-FAC)으로 대체한다.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/paper-review/" rel="tag"># paper review</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/21/Characterizing-Optimal-Mixed-Policies/" rel="prev" title="Characterizing Optimal Mixed Policies">
      <i class="fa fa-chevron-left"></i> Characterizing Optimal Mixed Policies
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/08/18/Transportability-of-Causal-and-Statistical-Relations-A-Formal-Approach/" rel="next" title="Transportability of Causal and Statistical Relations: A Formal Approach">
      Transportability of Causal and Statistical Relations: A Formal Approach <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-Abstract"><span class="nav-text">0. Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Meta-Learning-Formulation"><span class="nav-text">2. Meta-Learning Formulation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Meta-Learning-as-Gradient-Based-Hyperparameter-Optimization"><span class="nav-text">2.1 Meta-Learning as Gradient-Based Hyperparameter Optimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Meta-Learning-as-Hierarchical-Bayesian-Inference"><span class="nav-text">2.2 Meta-Learning as Hierarchical Bayesian Inference</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Linking-Gradient-Based-Meta-Learning-amp-Hierarchical-Bayes"><span class="nav-text">3. Linking Gradient-Based Meta-Learning &amp; Hierarchical Bayes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-MAML-as-Empirical-Bayes"><span class="nav-text">3.1 MAML as Empirical Bayes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-The-Prior-over-Task-Specific-Parameters"><span class="nav-text">3.2 The Prior over Task-Specific Parameters</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Improving-MAML"><span class="nav-text">4. Improving MAML</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Laplace%E2%80%99s-Method-of-Integration"><span class="nav-text">4.1 Laplace’s Method of Integration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Using-Curvature-Information-to-Improve"><span class="nav-text">4.2 Using Curvature Information to Improve</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">JaeHyun Lee</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JaeHyun Lee</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://leequant761.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://leequant761.github.io/2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/";
    this.page.identifier = "2021/08/01/Recasting-Gradient-Based-Meta-Learning-as-Hierarchical-Bayes/";
    this.page.title = "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://leequant761.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
