<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"leequant761.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="0. AbstractDTR은 decision rule들의 sequence이고 환자의 이전 치료와 covariate history에 기반해 어떻게 치료할 지를 명시한다. DTR은 만성(chronic) 질환 관리에 효과적이고, 개인화된 의사결정에 핵심적이다. 이 논문에선, observational data도 활용 가능할 때, optimal DTR을 찾는 onl">
<meta property="og:type" content="article">
<meta property="og:title" content="Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes">
<meta property="og:url" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/index.html">
<meta property="og:site_name" content="Study Repo">
<meta property="og:description" content="0. AbstractDTR은 decision rule들의 sequence이고 환자의 이전 치료와 covariate history에 기반해 어떻게 치료할 지를 명시한다. DTR은 만성(chronic) 질환 관리에 효과적이고, 개인화된 의사결정에 핵심적이다. 이 논문에선, observational data도 활용 가능할 때, optimal DTR을 찾는 onl">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614085211721.png">
<meta property="og:image" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614085211721.png">
<meta property="og:image" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614125233956.png">
<meta property="og:image" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614134419582.png">
<meta property="og:image" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614085211721.png">
<meta property="og:image" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210616181417109.png">
<meta property="og:image" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210616184352480.png">
<meta property="article:published_time" content="2021-07-04T16:02:15.000Z">
<meta property="article:modified_time" content="2021-11-15T13:50:51.990Z">
<meta property="article:author" content="JaeHyun Lee">
<meta property="article:tag" content="paper review">
<meta property="article:tag" content="dynamic treatment regime">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614085211721.png">

<link rel="canonical" href="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes | Study Repo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Study Repo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="JaeHyun Lee">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Study Repo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-05 01:02:15" itemprop="dateCreated datePublished" datetime="2021-07-05T01:02:15+09:00">2021-07-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-15 22:50:51" itemprop="dateModified" datetime="2021-11-15T22:50:51+09:00">2021-11-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Causality/" itemprop="url" rel="index"><span itemprop="name">Causality</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Causality/Reinforcement/" itemprop="url" rel="index"><span itemprop="name">Reinforcement</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="0-Abstract"><a href="#0-Abstract" class="headerlink" title="0. Abstract"></a>0. Abstract</h1><p>DTR은 decision rule들의 sequence이고 환자의 이전 치료와 covariate history에 기반해 어떻게 치료할 지를 명시한다.</p>
<p>DTR은 만성(chronic) 질환 관리에 효과적이고, 개인화된 의사결정에 핵심적이다.</p>
<p>이 논문에선, observational data도 활용 가능할 때, optimal DTR을 찾는 online RL 문제를 다룬다.</p>
<ul>
<li>이전 history를 사용하지 않고서 online setting에서 near-optimal DTR에 도달하는 adaptive algorithm을 개발했다.</li>
<li>Confounded observational data로부터 유의미한 regret의 upper/lower bound를 도출한다.</li>
<li>이들을 엮어서 confounded observational data도 활용하여 optimal DTR을 학습하는 RL algorithm을 개발했다.</li>
</ul>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>의료 실무에서 환자는 일반적으로 여러단계에 걸쳐 치료를 받는다.</p>
<p>의사는 반복적으로 환자의 state(바이러스 수치, 진단결과)에 맞춰서 treatment를 조정하게 된다.</p>
<p>DTR은 longitudinal setting에서 personalized treatment의 매력적인 framework를 제공한다.</p>
<p>DTR은 adaptive treatment strategy, treatment policy로도 알려져있다.</p>
<p>DTR은 암, 당뇨, 정신병과 같은 만성질환의 개인화된 치료에 효과적이다.</p>
<hr>
<p><strong>예시</strong></p>
<img src="/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614085211721.png" class="">
<p>알코올 의존증 환자들의</p>
<ul>
<li>condiition $S_1$에 기반하여</li>
<li>처방 $X_1$을 하고  </li>
<li>효과가 있었냐 없었냐 $S_2$로 구분하고 </li>
<li>초기 치료를 유지할 지 바꿀 지 $X_2$ 결정하여</li>
<li>환자의 12개월 간의 금주 비율 $Y$이 어떨 지에 대해 관심이 있다.</li>
<li>$U$는 unobserved confounder를 나타내고 환자에 대해 모르는 모든 요인들에 대한 요약이다.</li>
</ul>
<blockquote>
<p>DTR은 이들 전체의 SCM을 가르키고 treatment policy는 $X$를 결정짓는 SCM의 일부분을 가르킨다.</p>
</blockquote>
<hr>
<p>Policy learning in DTR은 $Y$를 최대화하는 optimal policy를 찾는 것이다.</p>
<p>Main challenge는 DTR의 parameter를 모르기 때문에 $E_{\boldsymbol{\pi}}\left[Y\right]$를 직접적으로 계산할 수 없다.</p>
<hr>
<p>Causal Inference 쪽에서 대부분의 work은 causal assumption과 finite observational data를 갖고서 $E_{\boldsymbol{\pi}}\left[Y\right]$를 identify하는 것에 집중한다.</p>
<blockquote>
<p>optimal policy를 찾진 않음!</p>
</blockquote>
<p>Unobserved confounder가 없으면($\approx$ conditional ignorability), sequential backdoor criterion으로 estimate할 수 있다.</p>
<p>Ignorability가 유지될 때, $E_{\boldsymbol{\pi}}\left[Y\right]$를 estimate하는 효율적인 방법들이 있다.</p>
<ol>
<li>Propensity Score [26]</li>
<li>Inverse probability of treatment weighting [21, 25]</li>
<li>Q-learning [31, 20]</li>
</ol>
<blockquote>
<p>Randomized experiment로 estimate하는 논문도 있으나 비용적인 문제가 있다.</p>
</blockquote>
<hr>
<p>RL은 exploration과 exploitation의 balancing으로 효율적으로 policy learning을 한다.</p>
<blockquote>
<p>DTR learning이라고 표현됐는데, Model based RL이라면 맞는 표현일지도…?</p>
</blockquote>
<p>RL은 일반적인 MDP 셋팅에서 잘 작동한다.</p>
<p>이의 variation으로는 multi-armed bandits, partially-observable MDP, factored MDPs가 있다.</p>
<hr>
<p>우리의 focus는 observational data도 활용하면서 policy를 학습하는 것이다.</p>
<img src="/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614085211721.png" class="">
<p>CI 관점에서 어려운 것은 unobserved confounder 때문이다.</p>
<p>RL 관점에서 어려운 것은 이전의 treatment $X_1$이 최종 outcome $Y$에 영향을 주는 구조이다.</p>
<p>비록 Thompson Samling과 같은 heuristic approach가 있으나, 모든 observational data를 망각(oblivious)하기 때문에 optimal은 아니다.</p>
<blockquote>
<p>이 부분은 잘 모르는 데 다시 보도록 하자. see [35]</p>
</blockquote>
<hr>
<p>이 논문은 이러한 어려움들을 극복해내는 것이다.</p>
<p>이 문제를 large confounded observational data를 활용하면서 unknown DTR을 최적화할 수 있는 RL strategy를 도입하여 해결하겠다.</p>
<p><strong>Contribution</strong></p>
<ol>
<li>UC-DTR : observational data없이, DTR setting에서 near-optimal regret bound에 도달하는 알고리즘</li>
<li>Upper/Lower bound : DTR 구조에 기반해서(Thm 5, 6), observational data를 활용한 bound 도출</li>
<li>$\text { UC }^{c}-\text { DTR }$: 효율적으로 bound를 활용하여 online setting에서 학습을 가속화한다.</li>
</ol>
<p>결과에 대한 검증은 randomly generated DTR과 암치료 데이터에서 이루어진다.</p>
<h2 id="1-1-Preliminaries"><a href="#1-1-Preliminaries" class="headerlink" title="1.1 Preliminaries"></a>1.1 Preliminaries</h2><p>skip</p>
<h1 id="2-Optimizing-Dynamic-Treatment-Regimes"><a href="#2-Optimizing-Dynamic-Treatment-Regimes" class="headerlink" title="2. Optimizing Dynamic Treatment Regimes"></a>2. Optimizing Dynamic Treatment Regimes</h1><p><strong>Definition 1</strong> (DTR의 SCM 버전)</p>
<p>​    DTR is a SCM $\langle\boldsymbol{U}, \boldsymbol{V}, \boldsymbol{F}, P(\boldsymbol{u})\rangle$ where $\boldsymbol{V}=\left\{\overline{\boldsymbol{X}}_{K}, \overline{\boldsymbol{S}}_{K}, Y\right\}$</p>
<blockquote>
<p>$K$는 total stage를 의미함</p>
</blockquote>
<p>​    For $k=1,  \cdots , K$</p>
<ol>
<li>$X_k$ is a finite decision decided by a behavior policy $x_{k} \leftarrow f_{k}\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}, \boldsymbol{u}\right)$</li>
<li>$S_k$ is a finite state decided by a transition function $s_{k} \leftarrow \tau_{k}\left(\overline{\boldsymbol{x}}_{k-1}, \overline{\boldsymbol{s}}_{k-1}, \boldsymbol{u}\right)$</li>
<li>$Y$ is the primary outcome at the final stage $K$ decided by a reward function $y \leftarrow r\left(\overline{\boldsymbol{x}}_{K}, \overline{\boldsymbol{s}}_{K}, \boldsymbol{u}\right)$</li>
</ol>
<hr>
<p>DTR $M^*$는 observational distribution $P\left(\overline{\bar{x}}_{K}, \overline{\boldsymbol{s}}_{K}, y\right)$를 유도한다.</p>
<p>Policy들의 collection은 policy space $\Pi$를 정의한다.</p>
<p>Policy $\boldsymbol\pi$는 interventional distribution over $\overline{\boldsymbol{X}}_{K}, \overline{\boldsymbol{S}}_{K}, Y$를 유도한다.</p>
<script type="math/tex; mode=display">
P_{\pi}\left(\overline{\boldsymbol{x}}_{K}, \overline{\boldsymbol{s}}_{K}, y\right)=P_{\overline{\boldsymbol{x}}_{K}}\left(y \mid \overline{\boldsymbol{s}}_{K}\right) \prod_{k=0}^{K-1} P_{\overline{\boldsymbol{x}}_{k}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right) \pi_{k+1}\left(x_{k+1} \mid \overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right)</script><img src="/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614125233956.png" class="">
<p>$M^{\star} + \pi$에 따라 expected reward $V_{\boldsymbol{\pi}}\left(M^{\star}\right)=E_{\boldsymbol{\pi}}[Y]$ 이 주어진다.</p>
<p>우리는 optimal policy $\pi^{\star} := \arg \max _{\boldsymbol{\pi} \in \boldsymbol{\Pi}} V_{\boldsymbol{\pi}}\left(M^{\star}\right)$ 를 찾는 게 목표이다.</p>
<blockquote>
<p>Decision theory에서 random policy들만으로 구성된 policy space나 deterministic policy로만 구성된 policy space나 optimal value가 같다는 게 알려져있으니 편의를 위해 deterministic policy만으로 구성된 policy space를 사용하겠다.</p>
</blockquote>
<hr>
<p>Unknown DTR $M^{*}$를 학습하고 싶다.</p>
<p>Agent(의사)는 반복된 experiment들로 학습한다.</p>
<p>이 때 각 episode $t$는 DTR과 policy의 realization이다.</p>
<p>$T$번 째 episode까지의 regret을 $R(T)=\sum_{t=1}^{T}\left(V_{\pi^{*}}\left(M^{\star}\right)-Y^{t}\right)$로 표기하자.</p>
<p>Agent가 optimal policy $\pi^*$에 수렴함에 따라 $\lim _{T \rightarrow \infty} E[R(T)] / T=0$을 기대한다.</p>
<h2 id="2-1-The-UC-DTR-Algorithm"><a href="#2-1-The-UC-DTR-Algorithm" class="headerlink" title="2.1 The UC-DTR Algorithm"></a>2.1 The UC-DTR Algorithm</h2><p>UC-DTR은 unknown DTR을 최적화하는 RL 알고리즘이다.</p>
<p>Domain $\boldsymbol{\mathcal{S}}, \boldsymbol{\mathcal{X}}$ 상의 지식만 갖고서 total regret의 near optimal에 도달한다는 것을 증명하겠다.</p>
<blockquote>
<p>도메인 상의 지식이 뭘 말하는 건지 아직 모름</p>
</blockquote>
<img src="/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614134419582.png" class="">
<p><strong>Step 1~3</strong> : 이전 에피소드들에서 얻어진 experimental samples들로 empirically estimate</p>
<p><strong>Step 4</strong> : 타당한(emprical에서 크게 벗어나지 않은) DTR 후보들의 집합 $\mathcal{M}_{t}$ 정의</p>
<p><strong>Step 5</strong> : 가장 optimistic한 DTR $M_t \in \mathcal{M}_{t}$ 에서 optimal policy $\boldsymbol{\pi}_{t}$ 를 찾기</p>
<hr>
<h3 id="2-1-1-Finding-Optimistic-DTRs"><a href="#2-1-1-Finding-Optimistic-DTRs" class="headerlink" title="2.1.1 Finding Optimistic DTRs"></a>2.1.1 Finding Optimistic DTRs</h3><p>Bellman equation은 DTR이 고정됐을 때 optimal policy를 찾는 걸 가능하게 해준다.</p>
<p>결국에 Step 5에서 가장 중요한 건 optimistic DTR을 찾으면 된다.</p>
<p>이 문제를 풀기위해 standard dynamic programming planner[5]를 확장하는 방법을 도입한다.</p>
<hr>
<p>우선 continuous decision space $\overline{\boldsymbol{\mathcal{X}}}^{+}=\overline{\boldsymbol{\mathcal{X}}}_{K}^{+}$를 고려하자.</p>
<blockquote>
<p>original $\overline{\boldsymbol{\mathcal{X}}}_{K}$는 가산+이산 set임.</p>
</blockquote>
<p>${}^{\forall} \overline{\boldsymbol{x}}_{k} \in \overline{\boldsymbol{\mathcal{X}}}_{k}$, $^{\exists}  \overline{\boldsymbol{x}}^+_{k} \in \overline{\boldsymbol{\mathcal{X}}}^+_{k}$ s.t.</p>
<script type="math/tex; mode=display">
P_{\overline{\boldsymbol{x}}_{k}^+}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right) = P_{\overline{\boldsymbol{x}}_{k}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right)
\\
E_{\overline{\boldsymbol{x}}_{K}^+}\left[Y \mid \overline{\boldsymbol{s}}_{K}\right] = E_{\overline{\boldsymbol{x}}_{K}}\left[Y \mid \overline{\boldsymbol{s}}_{K}\right]</script><p>$^\forall \boldsymbol\pi_{+} \text { on } M_{+}$, $^\exists\boldsymbol{\pi}_{t} \text{ on } M_t$ s.t.</p>
<script type="math/tex; mode=display">
P_{\boldsymbol\pi_{+}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right) = P_{\boldsymbol\pi_{t}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right)</script><p>그래서 Step 5는 extended DTR $M_+$에서 optimal policy를 찾는 문제와 동일하다.</p>
<p>Let $V^{\star}(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1})$ denote the optimal value $E_{\pi_{+}^{\star}}\left[Y \mid \overline{\boldsymbol s}_{k}, \overline{\boldsymbol x}_{k-1}\right]$.</p>
<p>Then, the Bellman equation on $M_+$ for $k=1, \cdots, K-1$ is defined as follows:</p>
<script type="math/tex; mode=display">
\begin{aligned}
V^{*}\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right)&= \max _{x_{k}}\left\{\max _{P_{\overline{\boldsymbol{x}}_{k}}\left(\cdot \mid \overline{\boldsymbol{s}}_{k}\right) \in \mathcal{P}_{k}}\left\{\sum_{s_{k+1}} V^{*}\left(\overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right) P_{\overline{\boldsymbol{x}}_{k}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right)\right\}\right\}

\\
V^{*}\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K-1}\right)&=  \max _{x_{K}} \max_{E_{\overline{\boldsymbol{w}}_{K}}\left[Y \mid \overline{\boldsymbol{s}}_{K}\right] \in \mathcal{R}}  E_{\overline{\boldsymbol{x}}_{K}}\left[Y \mid \overline{\boldsymbol{s}}_{K}\right]
\end{aligned}</script><p>where $\mathcal{R}, \mathcal{P}_{k}$ are the convex polytope of parameters $E_{\overline{\boldsymbol{x}}_{K}}\left[Y \mid \overline{\boldsymbol{s}}_{K}\right], P_{\overline{\boldsymbol{x}}_{k}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right)$</p>
<p>이는 convex polytope 상에서의 linear programming으로 해결될 수 있다.</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
V_{\boldsymbol \pi}( \overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}) &= E_{\boldsymbol \pi} [Y \mid \overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}]
\\
&= \sum_{s_{k+1}}E_{\boldsymbol \pi} [Y \mid \overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k-1}] P_{\overline{\boldsymbol{x}}_{k}} (s_{k+1} \mid \overline{\boldsymbol{s}}_{k}) \quad \because\text{$\boldsymbol \pi$ is deterministic}
\\
&= \sum_{s_{k+1}}E_{\boldsymbol \pi} [Y \mid \overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}] P_{\overline{\boldsymbol{x}}_{k}} (s_{k+1} \mid \overline{\boldsymbol{s}}_{k}) \quad \because\text{$\boldsymbol \pi$ is deterministic}
\\
&= \sum_{s_{k+1}}V_{\boldsymbol \pi}( \overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k})P_{\overline{\boldsymbol{x}}_{k}} (s_{k+1} \mid \overline{\boldsymbol{s}}_{k})
\end{aligned}</script><p>Let $V^{*}\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right) = \max_{\boldsymbol \pi} V_{\boldsymbol \pi} \left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right)$</p>
<p>Then $V^\star$ satisfies </p>
</blockquote>
<script type="math/tex; mode=display">
V^*_{\boldsymbol \pi}( \overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}) = \max _{x_k}\sum_{s_{k+1}}V^*_{\boldsymbol \pi}( \overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k})P_{\overline{\boldsymbol{x}}_{k}} (s_{k+1} \mid \overline{\boldsymbol{s}}_{k})</script><blockquote>
<p>논문의 max에 $\mathcal{R}, \mathcal{P}_{k}$가 추가된 이유는 dynamics를 모르기 때문에 empirical에서 크게 벗어나지 않은 optimistic DTR 하에서 최대화하려는 의도이다.</p>
</blockquote>
<h2 id="2-2-Theoretical-Analysis"><a href="#2-2-Theoretical-Analysis" class="headerlink" title="2.2 Theoretical Analysis"></a>2.2 Theoretical Analysis</h2><p>UC-DTR의 asymptotic behavior를 알아보자.</p>
<p>모든 증명들은 appendix에 제공된다.</p>
<hr>
<p><strong>Theorem 1</strong> (Upper bound)</p>
<p>​    The regret of UC-DTR is bounded by w/ probability $\geq 1-\delta$</p>
<script type="math/tex; mode=display">
R(T) \leq12 K \sqrt{|\boldsymbol{\mathcal{S}}||\boldsymbol{\mathcal{X}}| T \log (2 K|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| T / \delta)}+4 K \sqrt{T \log (2 T / \delta)}</script><hr>
<p><strong>Theorem 2</strong> (Gap-dependent upper bound)</p>
<p>​    Let $\mathbf{\Pi}^{-}$ denote the set of sub-optimal policies $\{\boldsymbol{\pi} \in \boldsymbol{\Pi}: V_{\boldsymbol{\pi}}\left(M^{\star}\right)&lt;V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)\}$</p>
<p>​    Let $\Delta_{\boldsymbol{\pi}}=V_{\boldsymbol{\pi}^{<em>}}\left(M^{</em>}\right)-V_{\boldsymbol{\pi}}\left(M^{*}\right)$</p>
<p>​    Then, the expected regret of UC-DTR with parameter $\delta=\frac{1}{T}$ is bounded by</p>
<script type="math/tex; mode=display">
E[R(T)] \leq \max _{\boldsymbol{\pi} \in \Pi^{-}}\left\{\frac{33^{2} K^{2}|\boldsymbol{\mathcal{S}}||\boldsymbol{\mathcal{X}}| \log (T)}{\Delta_{\pi}}+\frac{32}{\Delta_{\pi}^{3}}+\frac{4}{\Delta_{\pi}}\right\}+1</script><hr>
<p><strong>Theorem 3</strong> (Lower bound)</p>
<p>​    For any algorithm, there is a DTR $M$ with horizon $K$ s.t.</p>
<script type="math/tex; mode=display">
E[R(T)] \geq 0.05 \sqrt{|\mathcal{S} \| \mathcal{X}| T} \quad \text{where $T\geq|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}|$}</script><hr>
<p>Theorem 1의 upper bound $\tilde{\mathcal{O}}(K \sqrt{|\boldsymbol{\mathcal{S}}||\boldsymbol{\mathcal{X}}| T})$가 이론적으로 도달할 수 있는 lower bound $\Omega(\sqrt{|\boldsymbol{\mathcal{S}}||\boldsymbol{\mathcal{X}}| T})$와 비슷하므로 UC-DTR은 near-optimal이다.</p>
<h1 id="3-Learning-from-Confounded-Observations"><a href="#3-Learning-from-Confounded-Observations" class="headerlink" title="3. Learning from Confounded Observations"></a>3. Learning from Confounded Observations</h1><p>Theorem 1~3은 DTR에서 online learning의 bound 계산에 $\boldsymbol{\mathcal S}, \boldsymbol{\mathcal X}$의 차원을 사용한다.</p>
<p>만약 이들이 high dimensional일 경우, cumulative regret $R(T)$는 커질 것이다.</p>
<p>High dimensional에서의 문제를 다루기 위해선…</p>
<hr>
<p>가장 자연스러운 방법은 다른 agent들이 그 환경에서 행동하면서 얻어진 abundant observational data를 활용하는 것이다.</p>
<p>UC-DTR은 observational distribution $P\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)$에서의 지식을 활용하지 못한다.</p>
<p>Observational sample들을 활용하여 online learner의 performance가 향상될 수 있도록하는 procedure를 보이겠다.</p>
<hr>
<p>만약 $\overline{\boldsymbol{S}}_{K}$가 $\overline{\boldsymbol{X}}_{K}, Y$에 대한 sequential backdoor criterion을 만족할 때, ${P}_{\overline{\boldsymbol{x}}_{k}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right)$와 $E_{\overline{\boldsymbol{x}}_{K}}\left[Y \mid \bar{s}_{k}\right]$는 identifiable하다.</p>
<p>그러면 optimal policy는 Q-learning같은 standard off-policy learning으로 구할 수 있다. See [7] Sec 3.5</p>
<img src="/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210614085211721.png" class="">
<p>하지만 위처럼 sbc가 깨지는 경우 non identifiability 문제가 발생한다.</p>
<hr>
<p><strong>Theorem 4</strong> (non-identifiable할 경우)</p>
<p>​    Given $P\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)&gt;0$, $^\exists \text{DTR } M_1, M_2$ s.t.</p>
<ul>
<li>$P^{M_{1}}\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)= P^{M_{2}}\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)=P\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)$</li>
<li>but</li>
<li>$\begin{equation}<br>P_{\overline{\boldsymbol{x}}_{K}}^{M_{1}}\left(\overline{\boldsymbol{s}}_{K}, y\right) \neq P_{\overline{\boldsymbol{x}}_{K}}^{M_{2}}\left(\overline{\boldsymbol{s}}_{K}, y\right)<br>\end{equation}$</li>
</ul>
<h2 id="3-1-Bounds-and-Partial-Identification-in-DTRs"><a href="#3-1-Bounds-and-Partial-Identification-in-DTRs" class="headerlink" title="3.1 Bounds and Partial Identification in DTRs"></a>3.1 Bounds and Partial Identification in DTRs</h2><p>${P}_{\overline{\boldsymbol{x}}_{k}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right)$ 와 $E_{\overline{\boldsymbol{x}}_{K}}\left[Y \mid \bar{s}_{k}\right]$ 에 bound를 주는 방식으로 partially identify한다.</p>
<hr>
<p><strong>Lemma 1 </strong>(Causal gap이 Observational gap보다 작다.)</p>
<p>​    For a DTR with $P\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)$,</p>
<script type="math/tex; mode=display">
\begin{equation}
P_{\overline{\boldsymbol{x}}_{k}}\left(\overline{\boldsymbol{s}}_{k+1}\right)-P_{\overline{\boldsymbol{x}}_{k}}\left(\overline{\boldsymbol{s}}_{k}\right) \leq P\left(\overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right)-P\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k}\right)
\end{equation}</script><hr>
<p>Lemma 1은 transition probability의 bound를 유도하는 데 사용된다.</p>
<hr>
<p><strong>Theorem 5</strong> (Transition probabilities에 대한 bound)</p>
<script type="math/tex; mode=display">
\begin{equation}
\frac{P\left(\overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right)}{\Gamma\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right)} \leq P_{\overline{\boldsymbol{x}}_{k}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right) \leq \frac{\Gamma\left(\overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right)}{\Gamma\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right)}
\end{equation}</script><p>​    where </p>
<ul>
<li>$\Gamma\left(\overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right)=P\left(\overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right)-P\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k}\right)+\Gamma\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right)$</li>
<li>$\Gamma\left(s_{1}\right)=P\left(s_{1}\right)$</li>
</ul>
<hr>
<p>Theorem 5는 DTR의 functional relationship을 활용하여 bound를 제시하였고, 이는 이전에 알려진 bound [17, 3, 39] 보다 낫다.</p>
<p>Let $\left[a_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right), b_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right)\right] = \left[ \frac{P\left(\overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right)}{\Gamma\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right)},\frac{\Gamma\left(\overline{\boldsymbol{s}}_{k+1}, \overline{\boldsymbol{x}}_{k}\right)}{\Gamma\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right)} \right]$</p>
<hr>
<p><strong>Theorem 6</strong> ($\left[a_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right), b_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right)\right] $는 optimal bound이다.)</p>
<p>​    There exists DTRs $M_{1}, M_{2}$ s.t.</p>
<ul>
<li>$P^{M_{1}}\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)=P^{M_{2}}\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)=P\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}, y\right)$</li>
<li>$P_{\overline{\boldsymbol{x}}_{k}}^{M_{1}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right)=a_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right)$</li>
<li>$P_{\overline{\boldsymbol{x}}_{k}}^{M_{2}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right)=b_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right)$</li>
</ul>
<hr>
<p><strong>Corollary 1</strong></p>
<script type="math/tex; mode=display">
\frac{E\left[Y \mid \bar{s}_{K}, \overline{\boldsymbol{x}}_{K}\right] P\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}\right)}{\Gamma\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K-1}\right)} \leq E_{\overline{\boldsymbol{x}}_{K}}\left[Y \mid \bar{s}_{k}\right] \leq 1-\frac{\left(1-E\left[Y \mid \overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}\right]\right) P\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}\right)}{\Gamma\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K-1}\right)}</script><h2 id="3-2-The-Causal-UC-DTR-Algorithm"><a href="#3-2-The-Causal-UC-DTR-Algorithm" class="headerlink" title="3.2 The Causal UC-DTR Algorithm"></a>3.2 The Causal UC-DTR Algorithm</h2><p>UC-DTR의 성능을 올리기 위해, Theorem 5와 Corollary 1에서 구한 bound를 활용하는 방법을 도입하겠다.</p>
<p>For $k=1, …, K-1$,</p>
<script type="math/tex; mode=display">
\begin{aligned}

\boldsymbol{\mathcal{C}}_{k}&=\left\{P  :P_{\overline{\boldsymbol{x}}_{k}}\left(s_{k+1} \mid \overline{\boldsymbol{s}}_{k}\right) \in\left[a_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right), b_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right)\right] ;\quad \forall (\bar{s}_{k+1}, \overline{\boldsymbol{x}}_{k})\right\}

\\
\\

\boldsymbol{\mathcal{C}}_{K}&=\left\{ \mathbb E  :\mathbb E_{\overline{\boldsymbol{x}}_{K}}\left[Y \mid \overline{\boldsymbol{s}}_{K}\right] \in\left[a_{\overline{\boldsymbol{x}}_{K}, \overline{\boldsymbol{s}}_{K}}, b_{\overline{\boldsymbol{x}}_{K}, \overline{\boldsymbol{s}}_{K}}\right]  ;\quad \forall (\bar{s}_{k+1}, \overline{\boldsymbol{x}}_{k})\right\}

\end{aligned}</script><p>$\boldsymbol{\mathcal C} =  \{ \boldsymbol{\mathcal C}_1, \boldsymbol{\mathcal C}_2, \cdots, \boldsymbol{\mathcal C}_K\}$ 를 <em>causal bounds</em> 라고 하자.</p>
<p>$\text {UC}^{c} \text { -DTR }$은 UC-DTR의 변형으로 possible DTR set에 causal bound를 만족해야 한다는 조건을 추가한다.</p>
<p>그래서 Linear programming 부분에 constraint를 추가하는 방식으로 풀 수 있다.</p>
<img src="/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210616181417109.png" class="">
<p>Let $\left|\boldsymbol{\mathcal{C}}_{k}\right|_{1}=\max _{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}} \sum_{s_{k+1}}\left|a_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right)-b_{\overline{\boldsymbol{x}}_{k}, \overline{\boldsymbol{s}}_{k}}\left(s_{k+1}\right)\right|$</p>
<p>Let $\left|\boldsymbol{\mathcal{C}}_{K}\right|_{1}=\max _{\overline{\boldsymbol{x}}_{K}, \overline{\boldsymbol{s}}_{K}}\left|a_{\overline{\boldsymbol{x}}_{K}, \overline{\boldsymbol{s}}_{K}}-b_{\overline{\boldsymbol{x}}_{K}, \overline{\boldsymbol{s}}_{K}}\right|$</p>
<p>Let $|\boldsymbol{\mathcal{C}}|_{1}=\sum_{k=1}^{K}\left|\boldsymbol{\mathcal{C}}_{k}\right|_{1}$</p>
<hr>
<p><strong>Theorem 7</strong></p>
<p>​    The regret of $\text {UC}^{c} \text { -DTR }$ is bounded by w/ probability $\geq 1-\delta$</p>
<script type="math/tex; mode=display">
R(T) \leq \min \left\{12 K \sqrt{|\boldsymbol{\mathcal{S}}||\boldsymbol{\mathcal{X}}| T \log (2 K|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| T / \delta)},\|\boldsymbol{\mathcal{C}}\|_{1} T\right\}+4 K \sqrt{T \log (2 T / \delta)}</script><hr>
<ul>
<li>만약 $T&lt;12^{2}\left|\boldsymbol{\mathcal{S}}|\boldsymbol{\mathcal{X}} | \log (2 K|\boldsymbol{\mathcal{S}} | \boldsymbol{\mathcal{X}}| T / \delta) /| \boldsymbol{\mathcal{C}} |_{1}^{2}\right.$ 이면,  UC-DTR보다 작은 bound이다.</li>
<li>$|\boldsymbol{\mathcal{C}}|_{1}$이 작을 수록 더 정보가 있다는 것이니, 더 나은 $T$도 더 크다는 걸 확인할 수 있다.</li>
</ul>
<hr>
<p><strong>Theorem 8</strong></p>
<p>​    The expected regret of $\text {UC}^{c} \text { -DTR }$ with parameter $\delta=\frac{1}{T}$ is bounded by</p>
<script type="math/tex; mode=display">
E[R(T)] \leq \max _{\boldsymbol{\pi} \in \Pi_{\boldsymbol{\mathcal{c}}}^{-}}\left\{\frac{33^{2} K^{2}| \boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| \log (T)}{\Delta_{\pi}}+\frac{32}{\Delta_{\pi}^{3}}+\frac{4}{\Delta_{\pi}}\right\}+1</script><hr>
<ul>
<li>$\mathbf{\Pi}_{\boldsymbol{C}}^{-} \subseteq \mathbf{\Pi}^{-}$ 이므로, UC-DTR보다 작은 bound이다.</li>
</ul>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><img src="/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/image-20210616184352480.png" class="">
<p>(c)는 Cancer and Leukemia Group B에서 실행된 two-stage clinical trial이다.</p>
<h1 id="Appendix-1"><a href="#Appendix-1" class="headerlink" title="Appendix 1"></a>Appendix 1</h1><h2 id="A-1-1-Proof-of-Theorems-1-to-3"><a href="#A-1-1-Proof-of-Theorems-1-to-3" class="headerlink" title="A.1.1 Proof of Theorems 1 to 3"></a>A.1.1 Proof of Theorems 1 to 3</h2><p>Let “an episode $t$ is $\epsilon$-bad” if $V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)-Y^t \geq \epsilon$</p>
<p>Let $T_\epsilon$ be the number of $\epsilon$-bad episodes in UC_DTR</p>
<p>Let $L_{\epsilon}$ be the indicies of the $\epsilon$-bad episodes up to $T$</p>
<p>Let $R_{\epsilon}(T)=\sum_{t \in L_{f}} V_{\pi^{\star}}\left(M^{\star}\right)-Y^{t}$</p>
<p>Let $N\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k}\right)=\sum_{t=1}^{T} I_{\overline{\boldsymbol{S}}_{k}^{t}=\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{X}}_{k}^{t}=\overline{\boldsymbol{x}}_{k}}$</p>
<p>Let $\mathcal{H}^{t}=\left\{\overline{\boldsymbol{X}}_{K}^{1}, \overline{\boldsymbol{S}}_{K}^{1}, Y^{1}, \ldots, \overline{\boldsymbol{X}}_{K}^{t}, \overline{\boldsymbol{S}}_{K}^{t}, Y^{t}\right\}$</p>
<hr>
<p><strong>Lemma 2</strong></p>
<p>​    Fix $\epsilon \in (0, 1)$. Then, </p>
<script type="math/tex; mode=display">
\sum_{t \in L_{e}}\left(E_{\bar{X}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-Y^{t}\right) \leq \sqrt{\frac{T_{\epsilon} \log (1 / \delta)}{2}} \quad \text{w/ pbt $\geq 1-\delta$}</script><p>(Proof)</p>
<p>​    Let $\boldsymbol{D}^{T} = \left\{\overline{\boldsymbol{X}}_{K}^{1}, \overline{\boldsymbol{S}}_{K}^{1}, \ldots, \overline{\boldsymbol{X}}_{K}^{T}, \overline{\boldsymbol{S}}_{K}^{T}\right\}$</p>
<p>​    Since $Y^t \in (0, 1)$ and $Y^t$ are conditionally independent, by applying Hoeffding’s inequality</p>
<script type="math/tex; mode=display">
\begin{aligned}
&P\left(\sum_{t \in L_{e}}\left(E_{\overline{\boldsymbol{x}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{s}}_{K}^{t}\right]-Y^{t}\right) \geq \sqrt{\frac{T_{\epsilon} \log (1 / \delta)}{2}} \mid \boldsymbol{d}^{T}\right) 

\\
=&P\left(\frac{1}{T_\epsilon}\sum_{t \in L_{e}}\left(E_{\overline{\boldsymbol{x}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{s}}_{K}^{t}\right]-Y^{t}\right) \geq \sqrt{\frac{\log (1 / \delta)}{2T_{\epsilon} }} \mid \boldsymbol{d}^{T}\right) \leq \exp(-2 T_{\epsilon}  \frac{\log (1 / \delta)}{2T_{\epsilon} } / (1-0)^2) =\delta
\end{aligned}</script><p>​    Therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(\sum_{t \in L_{e}}\left(E_{\overline{\boldsymbol{X}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-Y^{t}\right) \geq \sqrt{\frac{T_{\epsilon} \log (1 / \delta)}{2}} \right) =& 
\\
\sum_{\boldsymbol d^T} P\left(\sum_{t \in L_{e}}\left(E_{\overline{\boldsymbol{X}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-Y^{t}\right) \geq \sqrt{\frac{T_{\epsilon} \log (1 / \delta)}{2}} \mid \boldsymbol d^T \right) P(\boldsymbol d^T) \leq &
\\
\sum_{\boldsymbol d^T} \delta P(\boldsymbol d^T) = \delta&
\end{aligned}</script><hr>
<p><strong>Lemma 3</strong></p>
<p>​    Fix $\epsilon &gt; 0$. Then w/ pbt $\geq 1-\delta$</p>
<script type="math/tex; mode=display">
^{\forall}T > 1, \quad R_{\epsilon}(T) \leq 12 K \sqrt{|\mathcal{S} \| \mathcal{X}| T_{\epsilon} \log (2 K|\mathcal{S} \| \mathcal{X}| T / \delta)}+4 K \sqrt{T_{\epsilon} \log (2 T / \delta)}</script><p>(Proof)</p>
<p>​    Let $M^{\star}$ denote the underlying DTR.</p>
<p>​    $\mathcal M_t$ is a set of DTR instances s.t. for any $M \in \mathcal M_t$, its system dynamics satisfy</p>
<ul>
<li>$\left|P_{\overline{\boldsymbol{x}}_{k}}^{M}\left(\cdot \mid \bar{s}_{k}\right)-\hat{P}_{\overline{\boldsymbol{x}}_{k}}^{t}\left(\cdot \mid \bar{s}_{k}\right)\right|_{1} \leq \sqrt{\frac{6\left|\mathcal{S}_{k+1}\right| \log \left(2 K\left|\overline{\boldsymbol{\mathcal { S }}}_{k}\right|\left|\overline{\boldsymbol{\mathcal { X }}}_{k}\right| t / \delta\right)}{\max \left\{1, N^{t}\left(\bar{s}_{k}, \overline{\boldsymbol{x}}_{k}\right)\right\}}}$</li>
<li>$\left|E_{\overline{\boldsymbol{x}}_{K}}^{M}\left[Y \mid \overline{\boldsymbol{s}}_{K}\right]-\hat{E}_{\overline{\boldsymbol{x}}_{K}}^{t}\left[Y \mid \overline{\boldsymbol{s}}_{K}\right]\right| \leq \sqrt{\frac{2 \log (2 K|\mathcal{S} | \mathcal{X}| t / \delta)}{\max \left\{1, N^{t}\left(\overline{\boldsymbol{s}}_{K}, \overline{\boldsymbol{x}}_{K}\right)\right\}}}$</li>
</ul>
<p>​    <a href="">UCRL2, Appendix C.1</a>에서,</p>
<ul>
<li><p>$\varepsilon=\sqrt{\frac{2}{n} \log \left(\frac{2^{S} 20 S A t^{7}}{\delta}\right)} \leq \sqrt{\frac{14 S}{n} \log \left(\frac{2 A t}{\delta}\right)} \quad \text{and} \quad \varepsilon_{r}=\sqrt{\frac{1}{2 n} \log \left(\frac{120 S A t}{\delta}\right)} \leq \sqrt{\frac{7}{2 n} \log \left(\frac{2 S A t}{\delta}\right)}$</p>
</li>
<li><p>$\varepsilon=\sqrt{\frac{2 }{n} \log \left( \frac{2^{\left|\mathcal{S}_{k+1}\right|} 8K\left|\overline{\mathcal{S}}_{k}\right|\left|\overline{\mathcal{X}}_{k}\right| t^3} {\delta}\right)} \leq \sqrt{\frac{6 \mathcal S_{k+1}}{n} \log \left(\frac{2K\left|\overline{\mathcal{S}}_{k}\right|\left|\overline{\mathcal{X}}_{k}\right| t}{\delta}\right)} \quad \text{and} \quad \varepsilon_{r}=\sqrt{\frac{1}{2 n} \log \left(\frac{8 K^2 |\mathcal S| |\mathcal X| t^3}{\delta}\right)} \leq \sqrt{\frac{2}{n} \log \left(\frac{2K |\mathcal S| |\mathcal X| t}{\delta}\right)}$</p>
</li>
<li><p>로 바꾸면 $P\left(M^{\star} \notin \mathcal{M}_{t}\right) \leq \frac{\delta}{4 t^{2}}$</p>
<ul>
<li>(노테이션이 달라서 실수로 $t$를 timestep으로 착각하고 유도했었는데 비슷할 듯)</li>
</ul>
</li>
<li><p>모든 episode에서 $M^\star \in \mathcal M_t$일 확률은 $1-\frac{\delta}{2}$보다 크다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
P \left ( \cap_{t=1} ^\infty  \left\{ M^\star\in \mathcal M_t  \right\}\right) &= 1 - P \left ( \cup_{t=1} ^\infty  \left\{ M^\star \not\in \mathcal M_t  \right\}\right)
\\
&\geq 1 - \sum_{t=1}^\infty P \left (\left\{ M^\star \not\in \mathcal M_t  \right\}\right)
\\
&\geq 1 - \sum_{t=1}^\infty\frac{\delta}{4t^2}
\\
&\geq 1 - \frac{\delta}{2} \quad \because \sum_{t=1}^{\infty} \frac{1}{4 t^{2}} \leq \frac{\pi^{2}}{24} \delta<\frac{\delta}{2}
\end{aligned}</script></li>
<li><p>$\frac{\delta}{2}$만큼의 확률은 내주고 아래의 증명에서 모든 episode에서 $M^\star \in \mathcal M_t$인 경우만 고려하겠다.</p>
</li>
<li><script type="math/tex; mode=display">
\begin{aligned}
R_{\epsilon}(T) &=\sum_{t \in L_{f}} V_{\pi^{\star}}\left(M^{\star}\right)-Y^{t}
\\
=& \sum_{t \in L_{\epsilon}}\left(V_{\pi^{\star}}\left(M^{\star}\right)-E_{\overline{\boldsymbol{X}}_{K}^{t}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]\right) &(19)
\\
&+\sum_{t \in L_{\epsilon}}\left(E_{\overline{\boldsymbol{X}}_{K}^{t}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-E_{\overline{\boldsymbol{X}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]\right) &(20)
\\
&+\sum_{t \in L_{\epsilon}}\left(E_{\overline{\boldsymbol{X}}_{K}^{t}}\left[Y \mid \bar{S}_{K}^{t}\right]-Y^{t}\right)&(21)
\end{aligned}</script></li>
<li><p><strong>Bounding Eq. (19)</strong></p>
<ul>
<li>Let $V_{\boldsymbol{\pi}}\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1} ; M\right)=E_{\boldsymbol{\pi}}^{M}\left[Y \mid \overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k-1}\right]$</li>
<li>Let $V_{\boldsymbol{\pi}}\left(\overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k} ; M\right)=E_{\boldsymbol{\pi}}^{M}\left[Y \mid \overline{\boldsymbol{s}}_{k}, \overline{\boldsymbol{x}}_{k}\right]$</li>
<li><p>Since $M^{\star} \in \mathcal{M}_{t}$, $\quad V_{\boldsymbol{\pi}^{<em>}}\left(s_{1} ; M^{</em>}\right) \leq V_{\boldsymbol{\pi}_{t}}\left(s_{1} ; M_{t}\right)$</p>
</li>
<li><p>Since $\pi_{t}$ is deterministic, $\quad V_{\boldsymbol{\pi}_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k-1}^{t} ; M\right)=V_{\boldsymbol{\pi}_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M\right)$</p>
</li>
<li><p>Thus, we have</p>
</li>
</ul>
<script type="math/tex; mode=display">
V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)-E_{\overline{\boldsymbol{X}}_{K}^{\mathrm{i}}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right] \leq V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)-V_{\boldsymbol{\pi}^{\star}}\left(\overline{\boldsymbol{S}}_{1}^{t} ; M^{\star}\right)+V_{\boldsymbol{\pi}_{t}}\left(\overline{\boldsymbol{S}}_{1}^{t}, \overline{\boldsymbol{X}}_{1}^{t} ; M_{t}\right)-E_{\overline{\boldsymbol{X}}_{K}^{t}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]</script></li>
</ul>
<ul>
<li>Let $M_{t}(k)$ denote a combined DTR from $M^{\star}$ and $M_t$ s.t.</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
P_{\overline{\boldsymbol{x}}_{i}}^{M_{t}(k)}\left(s_{i+1} \mid \overline{\boldsymbol{s}}_{i}\right)& = P_{\overline{\boldsymbol{x}}_{i}}\left(s_{i+1} \mid \bar{s}_{i}\right) \quad \text{for $i=0, 1, \ldots, k-1$}
\\
P_{\overline{\boldsymbol{x}}_{i}}^{M_{t}(k)}\left(s_{i+1} \mid \overline{\boldsymbol{s}}_{i}\right) &= P^{M_t}_{\overline{\boldsymbol{x}}_{i}}\left(s_{i+1} \mid \bar{s}_{i}\right) \quad \text{for $i=k, k+1, \ldots, K-1$}
\end{aligned}</script><ul>
<li><p>Then,</p>
<ul>
<li>$P_{\boldsymbol{\pi}}^{M_{t}(k)}\left(\overline{\boldsymbol{x}}_{K}, \overline{\boldsymbol{s}}_{K}, y\right)=P_{\overline{\boldsymbol{x}}_{K}}^{M_{t}}\left(y \mid \overline{\boldsymbol{s}}_{K}\right) \prod_{i=0}^{k-1} P_{\overline{\boldsymbol{x}}_{i}}\left(s_{i+1} \mid \overline{\boldsymbol{s}}_{i}\right) \prod_{j=k}^{K-1} P_{\overline{\boldsymbol{x}}_{j}}^{M_{t}}\left(s_{i+1} \mid \overline{\boldsymbol{s}}_{j}\right) \prod_{l=1}^{K-1} \pi_{l+1}\left(x_{l+1} \mid \overline{\boldsymbol{s}}_{l+1}, \overline{\boldsymbol{x}}_{l}\right)$</li>
</ul>
</li>
<li><script type="math/tex; mode=display">
\begin{aligned}
V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)-E_{\overline{\boldsymbol{X}}_{K}^{\mathrm{i}}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right] &\leq V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)-V_{\boldsymbol{\pi}^{\star}}\left(\overline{\boldsymbol{S}}_{1}^{t} ; M^{\star}\right)+V_{\boldsymbol{\pi}_{t}}\left(\overline{\boldsymbol{S}}_{1}^{t}, \overline{\boldsymbol{X}}_{1}^{t} ; M_{t}\right)-E_{\overline{\boldsymbol{X}}_{K}^{t}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]
\\
& = V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)-V_{\boldsymbol{\pi}^{\star}}\left(\overline{\boldsymbol{S}}_{1}^{t} ; M^{\star}\right)
\\
&
+\sum_{k=1}^{K-1} V_{\pi_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k)}\right)-V_{\pi_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k+1)}\right)
\\
&
+\sum_{k=1}^{K-1} V_{\boldsymbol{\pi}_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k+1)}\right)-V_{\boldsymbol{\pi}_{t}}\left(\overline{\boldsymbol{S}}_{k+1}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k+1)}\right)
\\
&
=: \sum_{k=1}^{K-1} V_{\pi_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k)}\right)-V_{\pi_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k+1)}\right) + Z_{t}
\end{aligned}</script><ul>
<li><script type="math/tex; mode=display">
\begin{aligned}
&V_{\pi_{t}}\left(\bar{\boldsymbol S}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k)}\right)-V_{\pi_{t}}\left(\bar{\boldsymbol S}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k+1)}\right) \\
&=\sum_{s_{k+1}}\left(P^{M_{t}}\left(s_{k+1} \mid \bar{\boldsymbol S}_{k}, \overline{\boldsymbol{X}}_{k}\right)-P\left(s_{k+1} \mid \bar{\boldsymbol S}_{k}, \overline{\boldsymbol{X}}_{k}\right)\right) V_{\pi_{t}}\left(s_{k+1}, \bar{\boldsymbol S}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}\right) \\
&\leq\left\|P_{\overline{\boldsymbol{x}}_{k}}^{M_{t}}\left(\mid \bar{\boldsymbol s}_{k}\right)-P_{\overline{\boldsymbol{x}}_{k}}\left(\cdot \mid \bar{\boldsymbol s}_{k}\right)\right\| \max _{s_{k+1}} v_{k+1}\left(s_{k+1}, \overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}\right)
\\
&\leq 2 \sqrt{6\left|\mathcal{S}_{k+1}\right| \log \left(2 K\left|\overline{\boldsymbol {\mathcal{S}}}_{k}\right|\left|\overline{\boldsymbol {\mathcal{X}}}_{k}\right| T / \delta\right)} \frac{1}{\sqrt{\max \left\{1, N^{t}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t}\right)\right.}}
\end{aligned}</script></li>
<li><p>$\sum_{t \in L_{\epsilon}} \frac{1}{\sqrt{\max \left\{1, N^{t}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t}\right)\right\}}} \leq(\sqrt{2}+1) \sqrt{T_{\epsilon}\left|\overline{\boldsymbol{\mathcal { S }}}_{k}\right|\left|\overline{\boldsymbol{\mathcal { X }}}_{k}\right|} \qquad \text{in UCRL2, appendix D}$</p>
</li>
</ul>
</li>
<li><script type="math/tex; mode=display">
\begin{aligned}
&\sum_{t \in L_{\epsilon}} \sum_{k=1}^{K-1} V_{\boldsymbol{\pi}_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k)}\right)-V_{\boldsymbol{\pi}_{t}}\left(\overline{\boldsymbol{S}}_{k}^{t}, \overline{\boldsymbol{X}}_{k}^{t} ; M_{t}^{(k+1)}\right) \\

&\leq \sum_{k=1}^{K-1} 2(\sqrt{2}+1) \sqrt{6 T_{\epsilon}\left|\overline{\boldsymbol{\mathcal{S}}}_{k+1}\right|\left|\overline{\boldsymbol{\mathcal { X }}}_{k}\right| \log \left(2 K\left|\overline{\boldsymbol{\mathcal { S }}}_{k}\right|\left|\overline{\boldsymbol{\mathcal { X }}}_{k}\right| T / \delta\right)} \\

&\leq 2(\sqrt{2}+1)(K-1) \sqrt{6 T_{\epsilon}|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| \log (2 K|\boldsymbol {\mathcal{S}} \| \boldsymbol{\mathcal{X}}| T / \delta)} & (24)
\end{aligned}</script></li>
<li><p>Since $E \left [\left|Z_{t}\right| \right] \leq K$ and $E\left[Z_{t+1} \mid \mathcal{H}_{t}\right]=0$,</p>
<ul>
<li>$Z_t$ is a martingale difference sequence w.r.t. $\{ \overline{\boldsymbol{X}}_{K}^{t}, \overline{\boldsymbol{S}}_{K}^{t}, Y^{t}\}_{t=1, 2, \ldots}$</li>
</ul>
</li>
<li><script type="math/tex; mode=display">
\begin{aligned}
P\left(\sum_{t \in L_{\epsilon}} Z_{t} \leq K \sqrt{6 T_{\epsilon} \log (2 T / \delta)} \right) & \geq 1- \exp\left( -\frac{2K^2 6T_{\epsilon} \log \left( 2T / \delta\right)}{T_\epsilon (2K)^2}\right) & \because \text{Azuma-Hoeffding inequality}
\\
&= 1 - \frac{\delta^3}{8T^3}
\\
& \geq 1 - \frac{\delta}{8T^2}  &\because \delta^2 \leq T
\end{aligned}</script></li>
<li><p>Since $\sum_{T=1}^{\infty} \frac{\delta}{8 T^{2}} \leq \frac{\pi^{2}}{48} \delta&lt;\frac{\delta}{4}$,</p>
<ul>
<li><script type="math/tex; mode=display">
P\left(\sum_{t \in L_{\epsilon}} Z_{t} \leq K \sqrt{6 T_{\epsilon} \log (2 T / \delta)} \right) \geq 1-\frac{\delta}{4} \quad \text{for all }T \quad (25)</script></li>
</ul>
</li>
<li><p>Eqs. (24) and (25) combined give</p>
<ul>
<li><script type="math/tex; mode=display">
\begin{aligned}
&\sum_{t \in L_{\epsilon}}\left(V_{\boldsymbol{\pi}^{*}}\left(M^{*}\right)-E_{\overline{\boldsymbol{X}}_{K}^{t}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]\right) \\
&\leq 2(\sqrt{2}+1)(K-1) \sqrt{6 T_{\epsilon}|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| \log (2 K|\boldsymbol {\mathcal{S}} \| \boldsymbol{\mathcal{X}}| T / \delta)}+K \sqrt{6 T_{\epsilon} \log (2 T / \delta)}
\end{aligned}</script></li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Bounding Eq. (20)</strong></p>
<ul>
<li><script type="math/tex; mode=display">
\begin{aligned}
E_{\overline{\boldsymbol{X}}_{K}^{t}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-E_{\overline{\boldsymbol{X}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]

&\leq \left|E_{\overline{\boldsymbol{x}}_{K}}^{M_{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-\hat{E}_{\overline{\boldsymbol{x}}_{K}}^{t}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]\right|+\left|E_{\overline{\boldsymbol{X}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-\hat{E}_{\overline{\boldsymbol{x}}_{K}}^{t}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]\right|
\\
&\leq 2 \sqrt{2 \log (2 K|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| T / \delta)} \frac{1}{\sqrt{\max \left\{1, N^{t}\left(\overline{\boldsymbol{S}}_{K}^{t}, \overline{\boldsymbol{X}}_{K}^{t}\right)\right\}}}
\\

\\
& \leq 2(\sqrt{2}+1) \sqrt{2 T_{\epsilon}|\mathcal{S} \| \mathcal{X}| \log (2 K|\mathcal{S} \| \mathcal{X}| T / \delta)}  

\\ & \because \text{UCRL2 appendix D} \sum_{t \in L_{\epsilon}} \frac{1}{\sqrt{\max \left\{1, N^{t}\left(\overline{\boldsymbol{S}}_{K}^{t}, \overline{\boldsymbol{X}}_{K}^{t}\right)\right\}}} \leq(\sqrt{2}+1) \sqrt{T_{\epsilon}|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}|}
\end{aligned}</script></li>
</ul>
</li>
<li><p><strong>Bounding Eq. (21)</strong></p>
<ul>
<li><p>Lemma 2에서 $\sqrt{\frac{T_{\epsilon} \log (1 / \delta)}{2}} $ 대신에 $\sqrt{\frac{3 T_{\epsilon} \log (2 T / \delta)}{2}}$를 대입하면,</p>
<ul>
<li><script type="math/tex; mode=display">
\sum_{t \in L_{\epsilon}}\left(E_{\overline{\boldsymbol{X}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-Y^{t}\right) \leq \sqrt{\frac{3 T_{\epsilon} \log (2 T / \delta)}{2}} \quad \text{w/ pbt }\geq 1- \exp\left( -2T_{\epsilon}\frac{3 \log (2 T / \delta)}{2 T_{\epsilon}}\right)\geq 1-\frac{\delta}{8T^2}</script></li>
</ul>
</li>
<li><p>Since $\sum_{T=1}^{\infty} \frac{\delta}{8 T^{2}} \leq \frac{\pi^{2}}{48} \delta&lt;\frac{\delta}{4}$,</p>
<ul>
<li><script type="math/tex; mode=display">
\sum_{t \in L_{\epsilon}}\left(E_{\overline{\boldsymbol{X}}_{K}^{t}}\left[Y \mid \overline{\boldsymbol{S}}_{K}^{t}\right]-Y^{t}\right) \leq \sqrt{\frac{3 T_{\epsilon} \log (2 T / \delta)}{2}} \quad \text{w/ pbt }\geq 1 -\frac{\delta}{4}</script></li>
</ul>
</li>
</ul>
</li>
<li><p>Hence,</p>
<ul>
<li>$\begin{aligned}<br>R_{\epsilon}(T) &amp; \leq  2(\sqrt{2}+1)(K-1) \sqrt{6 T_{\epsilon}|\boldsymbol{\mathcal{S}} | \boldsymbol{\mathcal{X}}| \log (2 K|\boldsymbol {\mathcal{S}} | \boldsymbol{\mathcal{X}}| T / \delta)}+K \sqrt{6 T_{\epsilon} \log (2 T / \delta)} \\ &amp;+ 2(\sqrt{2}+1) \sqrt{2 T_{\epsilon}|\mathcal{S} | \mathcal{X}| \log (2 K|\mathcal{S} | \mathcal{X}| T / \delta)} + \sqrt{\frac{3 T_{\epsilon} \log (2 T / \delta)}{2}}  \quad \text{w/ pbt}\geq 1-\frac{\delta}{2}-\frac{\delta}{4}-\frac{\delta}{4}=1-\delta \\ &amp;\leq 12 K \sqrt{|\mathcal{S}||\mathcal{X}| T_{\epsilon} \log (2 K|\mathcal{S} | \mathcal{X}| T / \delta)}+4 K \sqrt{T_{\epsilon} \log (2 T / \delta)} \quad \text{: Simplification}<br>\end{aligned}$</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Theorem 1</strong> (Upper bound)</p>
<p>​    The regret of UC-DTR is bounded by w/ probability $\geq 1-\delta$</p>
<script type="math/tex; mode=display">
R(T) \leq12 K \sqrt{| \boldsymbol{\mathcal{S}}|| \boldsymbol{\mathcal{X}}| T \log (2 K|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| T / \delta)}+4 K \sqrt{T \log (2 T / \delta)}</script><p>(Proof)</p>
<p>​    Fix $\epsilon = 0$</p>
<p>​    Then, $T_{\epsilon} = T$ and $R_{\epsilon}(T) = R(T)$</p>
<p>​    By Lemma 3,</p>
<script type="math/tex; mode=display">
\begin{aligned}
R(T) \leq 12 K \sqrt{|\mathcal{S}||\mathcal{X}| T \log (2 K|\mathcal{S} \| \mathcal{X}| T / \delta)}+4 K \sqrt{T \log (2 T / \delta)} \quad \text{: Simplification}
\end{aligned}</script><hr>
<p><strong>Theorem 2</strong> (Gap-dependent upper bound)</p>
<p>​    Let $\mathbf{\Pi}^{-}$ denote the set of sub-optimal policies $\left\{\boldsymbol{\pi} \in \boldsymbol{\Pi}: V_{\boldsymbol{\pi}}\left(M^{\star}\right)&lt;V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)\right\}$</p>
<p>​    Let $\Delta_{\boldsymbol{\pi}}=V_{\boldsymbol{\pi}^{\star}}\left(M^{\star}\right)-V_{\boldsymbol{\pi}}\left(M^{\star}\right)$</p>
<p>​    Then, the expected regret of UC-DTR with parameter $\delta=\frac{1}{T}$ is bounded by</p>
<script type="math/tex; mode=display">
E[R(T)] \leq \max _{\boldsymbol{\pi} \in \Pi^{-}}\left\{\frac{33^{2} K^{2}|\boldsymbol{\mathcal{S}}||\boldsymbol{\mathcal{X}}| \log (T)}{\Delta_{\pi}}+\frac{32}{\Delta_{\pi}^{3}}+\frac{4}{\Delta_{\pi}}\right\}+1</script><p>(Proof)</p>
<p>​    By Lemma 3,</p>
<script type="math/tex; mode=display">
R_{\epsilon}(T) \leq 23 K \sqrt{|\boldsymbol{\mathcal{S}}||\boldsymbol{\mathcal{X}}| T_{\epsilon} \log (T / \delta)} \qquad \text{(w/ pbt $\geq 1-\delta$)}</script><p>​    Since $R_{\epsilon}(T) \geq \epsilon T_{\epsilon}$,</p>
<script type="math/tex; mode=display">
T_{\epsilon} \leq \frac{23^{2} K^{2}|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| \log (T / \delta)}{\epsilon^{2}} \qquad \text{(w/ pbt $\geq 1-\delta$)}</script><p>​    Therefore,</p>
<script type="math/tex; mode=display">
R_{\epsilon}(T) \leq \frac{23^{2} K^{2}| \boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| \log (T / \delta)}{\epsilon}  \qquad \text{(w/ pbt $\geq 1-\delta$)}</script><p>​    Let $\Delta = \arg \min _{\boldsymbol{\pi} \in \boldsymbol{\Pi}^{-}} \Delta_{\boldsymbol{\pi}}$</p>
<p>​    Let $\epsilon=\frac{\Delta}{2}$</p>
<p>​    Let $\delta=\frac{1}{T}$</p>
<p>​    Then,</p>
<script type="math/tex; mode=display">
\begin{aligned}
E\left[R_{\frac{\Delta}{2}}(T)\right] & \leq  \frac{23^{2} K^{2}|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| \log (T / \frac{1}{T})}{\frac{\Delta}{2}} \left(1- \delta\right) + T \delta & \because R(T) \leq T
\\
&\leq \frac{46^{2} K^{2}| \boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}| \log (T)}{\Delta}+1
\end{aligned}</script><blockquote>
<p>논문에선 $33^2$이라고 했는데 어떻게 derive 된 것?</p>
</blockquote>
<p>​    $\epsilon$-good episode들도 고려해보자.</p>
<p>​    Let $\tilde{R}_{\epsilon}(T)$ denote the regret in $\epsilon$-good episodes.</p>
<p>​    Let $\tilde T_{\epsilon}$ be the total number of $\epsilon$-good episodes.</p>
<p>​    Let $\tilde L_{\epsilon}$ be the indices of $\epsilon$-good episodes.</p>
<p>​    Consider the event $\{\tilde{T}_{\frac{\Delta}{2}}=t \}$</p>
<p>​    Then, $\{\tilde{T}_{\frac{\Delta}{2}}=t \}$ implies</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{R}_{\epsilon}(T)=\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} V_{\boldsymbol{\pi}^{*}}\left(M^{*}\right)-Y^{i} &\leq t \frac{\Delta}{2}
\\
&\Leftrightarrow
\\
\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} V_{\boldsymbol{\pi}^{*}}\left(M^{*}\right)-V_{\boldsymbol{\pi}_{i}}\left(M^{*}\right)-Y^{i} &\leq t \frac{\Delta}{2}-\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} V_{\boldsymbol{\pi}_{i}}\left(M^{*}\right)
\\
&\Leftrightarrow
\\
\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} \Delta_{\boldsymbol{\pi}_{i}}-Y^{i} &\leq t \frac{\Delta}{2}-\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} V_{\boldsymbol{\pi}_{i}}\left(M^{*}\right)
\\
&\Rightarrow
\\
\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} \Delta-Y^{i} &\leq t \frac{\Delta}{2}-\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} V_{\boldsymbol{\pi}_{i}}\left(M^{*}\right)
\\
&\Leftrightarrow
\\
\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} V_{\boldsymbol{\pi}_{i}}\left(M^{*}\right)-Y^{i} &\leq-t \frac{\Delta}{2}
\end{aligned}</script><p>​    Therefore,</p>
<script type="math/tex; mode=display">
\begin{aligned}
E\left[\tilde{R}_{\frac{\Delta}{2}}(T)\right] &\leq \frac{\Delta}{2} E\left[\tilde{T}_{\frac{\Delta}{2}}(T)\right] \leq \frac{\Delta}{2} \sum_{t=1}^{T} t P\left(\tilde{T}_{\frac{\Delta}{2}}=t\right)
\\
&\leq \frac{\Delta}{2} \sum_{t=1}^{T} t P\left(\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} V_{\boldsymbol{\pi}_{i}}\left(M^{*}\right)-Y^{i} \leq-t \frac{\Delta}{2}\right)
\\
&= \frac{\Delta}{2} \sum_{t=1}^{T} t P\left(\sum_{i \in \tilde{L}_{\frac{\Delta}{2}}} C_i \leq-t \frac{\Delta}{2}\right) &\text{where $C_{i}=V_{\boldsymbol{\pi}_{i}}\left(M^{*}\right)-Y^{t}$}
\\
&\leq \frac{\Delta}{2} \sum_{t=1}^{T} te^{-\frac{\Delta^{2} t}{8}} &\text{$\left|C_{t}\right|<1$, $E\left[C_{t+1} \mid \mathcal{H}^{t}\right]=0$, Azuma-Hoeffding }
\\
&\leq \frac{\Delta}{2} \frac{64}{\Delta^{4}}\left(\frac{\Delta^{2}}{8}+1\right) e^{-\frac{\Delta^{2}}{8}}
&\text{I don't know}
\\
& \leq \frac{32}{\Delta^{3}}+\frac{4}{\Delta}
\end{aligned}</script><blockquote>
<p>마지막에서 두번째 부등식은 유도하지 못했다.</p>
</blockquote>
<p>​    Hence,</p>
<script type="math/tex; mode=display">
\begin{aligned}
E[R(T)]&=E\left[R_{\frac{\Delta}{2}}(T)\right]+E\left[\tilde{R}_{\frac{\Delta}{2}}(T)\right]
\\
&\leq \frac{33^{2} K^{2}|\mathcal{S} \| \mathcal{X}| \log (T)}{\Delta}+\frac{32}{\Delta^{3}}+\frac{4}{\Delta}+1
\end{aligned}</script><hr>
<p><strong>Theorem 3</strong> (Lower bound)</p>
<p>​    For any algorithm, there is a DTR $M$ with horizon $K$ s.t.</p>
<script type="math/tex; mode=display">
E[R(T)] \geq 0.05 \sqrt{|\mathcal{S} \| \mathcal{X}| T} \quad \text{where $T\geq|\boldsymbol{\mathcal{S}} \| \boldsymbol{\mathcal{X}}|$}</script><p>(Proof)</p>
<p>​    <a href="">P. Auer, 2002</a>를 보고 다시 증명해보겠다.</p>
<hr>
<h2 id="A-1-2-Proofs-of-Theorems-4-to-6-Lemma-1-and-Corollary-2"><a href="#A-1-2-Proofs-of-Theorems-4-to-6-Lemma-1-and-Corollary-2" class="headerlink" title="A.1.2 Proofs of Theorems 4 to 6, Lemma 1, and Corollary 2"></a>A.1.2 Proofs of Theorems 4 to 6, Lemma 1, and Corollary 2</h2><p>이번 섹션에서, DTR의 transition probaility의 bound에 대한 증명을 하겠다.</p>
<p>증명은 Pearl의 counterfactual variable와 axioms of composition, effectiveness, and reversibility에 기반한다.</p>
<hr>
<p>$Y_{\boldsymbol{x}}(\boldsymbol{u})$은 potential outcome of $Y$ to intervention $do(\boldsymbol{x})$에서 situation $\boldsymbol U = \boldsymbol u$일 때의 해이다.</p>
<blockquote>
<p>Exogenous variable과 CF variable이 대응되므로 CF를 exogenous로 갖는 DTR을 생각해보자.</p>
<p>아래첨자 2단으로 사용하면 수식 변환이 안되어서 작성 중단</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/paper-review/" rel="tag"># paper review</a>
              <a href="/tags/dynamic-treatment-regime/" rel="tag"># dynamic treatment regime</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/05/Identifying-Dynamic-Sequential-Plans/" rel="prev" title="Identifying Dynamic Sequential Plans">
      <i class="fa fa-chevron-left"></i> Identifying Dynamic Sequential Plans
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/05/Stochastic-Sequential-Neural-Networks-with-Structured-Inference/" rel="next" title="Stochastic Sequential Neural Networks with Structured Inference">
      Stochastic Sequential Neural Networks with Structured Inference <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-Abstract"><span class="nav-text">0. Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Preliminaries"><span class="nav-text">1.1 Preliminaries</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Optimizing-Dynamic-Treatment-Regimes"><span class="nav-text">2. Optimizing Dynamic Treatment Regimes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-The-UC-DTR-Algorithm"><span class="nav-text">2.1 The UC-DTR Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-Finding-Optimistic-DTRs"><span class="nav-text">2.1.1 Finding Optimistic DTRs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Theoretical-Analysis"><span class="nav-text">2.2 Theoretical Analysis</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Learning-from-Confounded-Observations"><span class="nav-text">3. Learning from Confounded Observations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Bounds-and-Partial-Identification-in-DTRs"><span class="nav-text">3.1 Bounds and Partial Identification in DTRs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-The-Causal-UC-DTR-Algorithm"><span class="nav-text">3.2 The Causal UC-DTR Algorithm</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Experiments"><span class="nav-text">4 Experiments</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Appendix-1"><span class="nav-text">Appendix 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#A-1-1-Proof-of-Theorems-1-to-3"><span class="nav-text">A.1.1 Proof of Theorems 1 to 3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-1-2-Proofs-of-Theorems-4-to-6-Lemma-1-and-Corollary-2"><span class="nav-text">A.1.2 Proofs of Theorems 4 to 6, Lemma 1, and Corollary 2</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">JaeHyun Lee</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JaeHyun Lee</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://leequant761.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://leequant761.github.io/2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/";
    this.page.identifier = "2021/07/05/Near-Optimal-Reinforcement-Learning-in-Dynamic-Treatment-Regimes/";
    this.page.title = "Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://leequant761.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
